ğŸ“Œ Task 1: Data Preparation
ğŸ”¹ Objective

Prepare the Telco Customer Churn dataset for machine learning by handling missing values, encoding categorical variables, and ensuring data consistency.
ğŸ”¹ Steps Performed

Dataset Upload

Loaded the dataset (Telco_Customer_Churn_Dataset.csv) into Google Colab using Pandas.

Data Exploration

Checked dataset shape, column data types, and summary statistics.

Identified missing values and categorical variables.

Handling Missing Values

Numerical columns: Replaced missing values with the mean.

Categorical columns: Replaced missing values with the mode.

Encoding Categorical Variables

Binary categorical features (Yes/No): Applied Label Encoding (0/1).

Multi-category features: Applied One-Hot Encoding using pd.get_dummies() (drop_first=True to avoid multicollinearity).

Verification

Confirmed no missing values remained.

Verified all features were numeric and dataset was ML-ready.
ğŸ”¹ Tools & Libraries

Python

Pandas

NumPy

Scikit-learn

Google Colab
ğŸ”¹ Skills Practiced

âœ… Data preprocessing techniques
âœ… Handling missing values
âœ… Categorical variable encoding
âœ… Preparing data for ML pipeline

ğŸ”¹ Outcome

The dataset is now clean, consistent, and machine-learning ready, forming the foundation for further tasks like data splitting, feature selection, and model training.

ğŸ“Œ Task 2: Data Splitting
ğŸ”¹ Objective

Split the Telco Customer Churn dataset into training (80%) and testing (20%) sets for fair model training and evaluation, ensuring the split is representative of the target variable distribution.

ğŸ”¹ Steps Performed

Dataset Upload

Loaded the preprocessed dataset (Telco_Customer_Churn_Dataset.csv) into Google Colab.

Feature/Target Separation

Defined features (X) as all columns except Churn.

Defined target (y) as the Churn column.

Train-Test Split

Used scikit-learnâ€™s train_test_split function.

Parameters:

test_size=0.2 â†’ 20% for testing, 80% for training

random_state=42 â†’ ensures reproducibility

stratify=y â†’ maintains class balance of churn vs. non-churn customers

Basic Checks

Verified training/testing set shapes.

Confirmed target variable distribution is preserved in both splits.

ğŸ”¹ Tools & Libraries

Python

Pandas

NumPy

Scikit-learn

Google Colab

ğŸ”¹ Skills Practiced

âœ… Data splitting methodologies
âœ… Understanding training/testing dataset requirements
âœ… Ensuring reproducible and representative splits

ğŸ”¹ Outcome

The dataset was successfully divided into train (80%) and test (20%) sets, ensuring fair model evaluation and avoiding data leakage in future tasks.

ğŸ“Œ Task 3: Feature Selection & Correlation Analysis
ğŸ”¹ Objective

Identify the most influential features contributing to customer churn by applying correlation analysis and domain knowledge.

ğŸ”¹ Steps Performed

Dataset Upload

Loaded the preprocessed dataset (Telco_Customer_Churn_Dataset.csv) into Google Colab.

Feature-Target Correlation

Calculated correlation of all features with the target variable Churn.

Ranked features based on absolute correlation values.

Top Features Selection

Identified Top 10 most influential features affecting churn.

Focused on attributes like tenure, contract type, internet service, monthly charges, etc.

Correlation Heatmap

Created a heatmap visualization of the top 10 features.

Helped in spotting feature relationships and multicollinearity.

Domain Knowledge Validation

Verified feature importance with business understanding.

Example: Short tenure, high monthly charges, and contract type are logically linked to higher churn rates.

ğŸ”¹ Tools & Libraries

Python

Pandas

NumPy

Seaborn

Matplotlib

Scikit-learn

Google Colab

ğŸ”¹ Skills Practiced

âœ… Feature selection techniques
âœ… Correlation analysis
âœ… Visualization with heatmaps
âœ… Applying domain knowledge to validate attributes

ğŸ”¹ Outcome

Identified the Top 10 features influencing customer churn with statistical support and business logic.
This step ensures better model interpretability and improves prediction accuracy by focusing on the most relevant attributes.

ğŸ“Œ Task 6: Model Evaluation
ğŸ”¹ Objective

Evaluate the performance of a Logistic Regression model built on the Telco Customer Churn dataset using multiple classification metrics.

ğŸ”¹ Steps Performed

Dataset Upload

Loaded the preprocessed dataset (Telco_Customer_Churn_Dataset.csv) into Google Colab.

Train-Test Split

Used the 80/20 split from Task 2 for fair evaluation.

Ensured stratification to maintain class balance.

Model Training

Trained a Logistic Regression model using the selected features from Task 3.

Predictions

Made predictions on the test dataset.

Evaluation Metrics

Calculated:
âœ”ï¸ Accuracy Score
âœ”ï¸ Precision
âœ”ï¸ Recall
âœ”ï¸ F1-Score

Confusion Matrix Visualization

Plotted a confusion matrix heatmap in shaded pink for intuitive interpretation.

Highlighted true positives, true negatives, false positives, and false negatives.

ğŸ”¹ Tools & Libraries

Python

Pandas

NumPy

Scikit-learn

Seaborn

Matplotlib

Google Colab

ğŸ”¹ Skills Practiced

âœ… Model training with Logistic Regression
âœ… Classification metrics interpretation
âœ… Confusion matrix analysis
âœ… Visualization of evaluation results

ğŸ”¹ Outcome

Successfully evaluated a baseline model for customer churn prediction.
Gained insights into model performance beyond accuracy, emphasizing the role of precision, recall, and F1-score for better decision-making in churn prediction.
